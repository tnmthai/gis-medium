{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/forest\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/buildings\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/river\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/mobilehomepark\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/harbor\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/golfcourse\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/agricultural\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/runway\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/baseballdiamond\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/overpass\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/chaparral\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/tenniscourt\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/intersection\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/airplane\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/parkinglot\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/sparseresidential\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/mediumresidential\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/denseresidential\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/beach\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/freeway\n",
      "/Users/thaitran/Downloads/UCMerced_LandUse/Images/storagetanks\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labelnames = ['forest', 'buildings', 'river', 'mobilehomepark', 'harbor', 'golfcourse', 'agricultural', 'runway', 'baseballdiamond', 'overpass', 'chaparral', 'tenniscourt', 'intersection', 'airplane', 'parkinglot', 'sparseresidential', 'mediumresidential', 'denseresidential', 'beach', 'freeway', 'storagetanks']\n",
    "labels =[]\n",
    "classes = 21\n",
    "cur_path = os.getcwd()\n",
    "root_folder = '/Users/thaitran/Downloads/UCMerced_LandUse/Images/'\n",
    "\n",
    "\n",
    "for i in range(classes):\n",
    "    path = os.path.join(root_folder,labelnames[i])\n",
    "    print(path)\n",
    "    images = os.listdir(path)\n",
    "    \n",
    "\n",
    "    for a in images:\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(path + '//'+ a)\n",
    "            image = image.resize((64,64))\n",
    "            image = np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Normalize the pixel values\n",
    "data = data.astype('float32')/255.0\n",
    "\n",
    "# One-hot encoding of labels\n",
    "# from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (2, 2), input_shape=(64, 64, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/27 [==============================] - 2s 50ms/step - loss: 3.0646 - accuracy: 0.0470 - val_loss: 3.0220 - val_accuracy: 0.0214\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 1s 56ms/step - loss: 3.0098 - accuracy: 0.0696 - val_loss: 2.9546 - val_accuracy: 0.0905\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 1s 55ms/step - loss: 2.9691 - accuracy: 0.0839 - val_loss: 2.9076 - val_accuracy: 0.1095\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 2.8899 - accuracy: 0.1077 - val_loss: 2.8037 - val_accuracy: 0.1476\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 2s 56ms/step - loss: 2.8251 - accuracy: 0.1161 - val_loss: 2.6929 - val_accuracy: 0.1786\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 2.7494 - accuracy: 0.1446 - val_loss: 2.6017 - val_accuracy: 0.1905\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 1s 56ms/step - loss: 2.6755 - accuracy: 0.1589 - val_loss: 2.5161 - val_accuracy: 0.2190\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 2.5886 - accuracy: 0.1810 - val_loss: 2.4083 - val_accuracy: 0.2690\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 2.5642 - accuracy: 0.1792 - val_loss: 2.3448 - val_accuracy: 0.2905\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 2s 56ms/step - loss: 2.4549 - accuracy: 0.2024 - val_loss: 2.2855 - val_accuracy: 0.2810\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 2.4106 - accuracy: 0.2143 - val_loss: 2.2258 - val_accuracy: 0.3238\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 2s 56ms/step - loss: 2.3103 - accuracy: 0.2369 - val_loss: 2.1843 - val_accuracy: 0.3214\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 2.3199 - accuracy: 0.2256 - val_loss: 2.0939 - val_accuracy: 0.3810\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 2s 56ms/step - loss: 2.2953 - accuracy: 0.2494 - val_loss: 2.1325 - val_accuracy: 0.3429\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 2.2165 - accuracy: 0.2679 - val_loss: 2.0473 - val_accuracy: 0.4143\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 2.1776 - accuracy: 0.2530 - val_loss: 2.0013 - val_accuracy: 0.3929\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 2.1281 - accuracy: 0.2875 - val_loss: 2.0004 - val_accuracy: 0.3762\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 2.1233 - accuracy: 0.2833 - val_loss: 1.9276 - val_accuracy: 0.4286\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 2.0205 - accuracy: 0.3000 - val_loss: 1.9391 - val_accuracy: 0.4571\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 2.0708 - accuracy: 0.2881 - val_loss: 2.1015 - val_accuracy: 0.3738\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 2.0340 - accuracy: 0.3065 - val_loss: 1.9810 - val_accuracy: 0.3976\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.9837 - accuracy: 0.3119 - val_loss: 1.8609 - val_accuracy: 0.4619\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.9413 - accuracy: 0.3250 - val_loss: 1.8910 - val_accuracy: 0.4262\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.9647 - accuracy: 0.3173 - val_loss: 1.8631 - val_accuracy: 0.4500\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 1.9374 - accuracy: 0.3101 - val_loss: 1.8136 - val_accuracy: 0.4381\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.8795 - accuracy: 0.3369 - val_loss: 1.8891 - val_accuracy: 0.4476\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 2s 62ms/step - loss: 1.8661 - accuracy: 0.3375 - val_loss: 1.8192 - val_accuracy: 0.4548\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.8190 - accuracy: 0.3554 - val_loss: 1.7871 - val_accuracy: 0.4452\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.8504 - accuracy: 0.3429 - val_loss: 1.8724 - val_accuracy: 0.4619\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.7785 - accuracy: 0.3673 - val_loss: 1.8239 - val_accuracy: 0.4405\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 2s 60ms/step - loss: 1.7894 - accuracy: 0.3470 - val_loss: 1.8018 - val_accuracy: 0.4667\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.7644 - accuracy: 0.3696 - val_loss: 1.7648 - val_accuracy: 0.4548\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 2s 60ms/step - loss: 1.7250 - accuracy: 0.3756 - val_loss: 1.7859 - val_accuracy: 0.4857\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.7390 - accuracy: 0.3661 - val_loss: 1.7833 - val_accuracy: 0.4738\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 2s 60ms/step - loss: 1.7287 - accuracy: 0.3869 - val_loss: 1.7980 - val_accuracy: 0.4762\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.7059 - accuracy: 0.3714 - val_loss: 1.7532 - val_accuracy: 0.4810\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.6336 - accuracy: 0.4089 - val_loss: 1.7997 - val_accuracy: 0.4571\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.6875 - accuracy: 0.3940 - val_loss: 1.7192 - val_accuracy: 0.4905\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 2s 60ms/step - loss: 1.6381 - accuracy: 0.3958 - val_loss: 1.8008 - val_accuracy: 0.4429\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.6348 - accuracy: 0.4054 - val_loss: 1.7379 - val_accuracy: 0.5071\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 2s 60ms/step - loss: 1.6605 - accuracy: 0.3792 - val_loss: 1.7424 - val_accuracy: 0.4690\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.6034 - accuracy: 0.3946 - val_loss: 1.8102 - val_accuracy: 0.4524\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 2s 61ms/step - loss: 1.5915 - accuracy: 0.4095 - val_loss: 1.7701 - val_accuracy: 0.4714\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.6013 - accuracy: 0.4030 - val_loss: 1.7633 - val_accuracy: 0.4857\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 2s 62ms/step - loss: 1.6104 - accuracy: 0.3988 - val_loss: 1.7369 - val_accuracy: 0.4833\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.5578 - accuracy: 0.4149 - val_loss: 1.7563 - val_accuracy: 0.4619\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 2s 61ms/step - loss: 1.5469 - accuracy: 0.4143 - val_loss: 1.7755 - val_accuracy: 0.4929\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 2s 60ms/step - loss: 1.5170 - accuracy: 0.4423 - val_loss: 1.7722 - val_accuracy: 0.5024\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 2s 61ms/step - loss: 1.5303 - accuracy: 0.4321 - val_loss: 1.7595 - val_accuracy: 0.4738\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 1.5403 - accuracy: 0.4333 - val_loss: 1.7351 - val_accuracy: 0.4929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d4094070>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted class: river\n",
      "Class probabilities: [[1.94220319e-01 5.49669203e-05 3.76937896e-01 6.62979903e-04\n",
      "  1.77085828e-02 1.22873457e-02 8.04212317e-03 4.64845635e-02\n",
      "  1.48818102e-02 1.17990691e-02 2.34023391e-04 1.73909545e-01\n",
      "  9.48311947e-03 1.77453076e-05 4.55365778e-04 1.15495594e-02\n",
      "  1.14791483e-01 5.45046711e-03 1.05110055e-04 3.95038980e-04\n",
      "  5.28991746e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess a new image for prediction\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((64, 64))\n",
    "    image = np.array(image)\n",
    "    image = image.astype('float32') / 255.0\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "new_image_path = '/Users/thaitran/Downloads/UCMerced_LandUse/Images/river/river03.tif'\n",
    "new_image = preprocess_image(new_image_path)\n",
    "\n",
    "# Predict the class of the new image\n",
    "predicted_probs = model.predict(new_image)\n",
    "predicted_class = np.argmax(predicted_probs)\n",
    "\n",
    "# Get the corresponding label name\n",
    "predicted_label = labelnames[predicted_class]\n",
    "\n",
    "print(f\"Predicted class: {predicted_label}\")\n",
    "print(\"Class probabilities:\", predicted_probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geosense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
